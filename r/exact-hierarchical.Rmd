---
title: "Reinventing Test and Trace: Exact Hierarchical Model"
subtitle: "Using Random Testing and Perfect Tests"
output: html_notebook
runtime: shiny
---

```{r echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
library(purrr)
library(rstan)
library(shiny)
library(stringr)
library(tibble)
library(tidyr)
```

```{r echo=FALSE}
# Helper functions
logit <- function(x) log(x / (1 - x))
expit <- function(x) 1 / (1 + exp(-x))
```

```{r echo=FALSE}
NUM_CORES <- parallel::detectCores()
options(mc.cores = NUM_CORES)
rstan_options(auto_write = TRUE)
```

```{r echo=FALSE}
theme_set(theme_bw())
```

## Introduction

### Model Objectives and Assumptions

This notebook introduces a Bayesian model, implemented in Stan, for predicting the transmission rates of COVID-19 during various activities. The model is based on the following process:

- A random sample of the population are tested for Coronavirus.
- Each person in the sample fills out a survey detailing whether they visited one of multiple locations in the last week.
- This survey is combined with their test result to infer the infection rate at the surveyed locations and the base transmission rate.

In this notebook, we look at a simple variant of the model with many assumptions (which are relaxed in later versions at the expense of code/model complexity). That said, this approach goes beyond the base model by using a hierarchical structure, grouping locations of the same _class_ (e.g. cinemas, hairdressers) together.

We make the following assumptions:
- Our test/survey samples come from a random sample of the population (i.e. designed experiment rather than observational).
- Coronavirus tests are 100% accurate.
- Multiple visits to a location in one week are rare enough to be ignored or in cases (such as supermarkets) where multiple visits are expected, the distribution of number of visits is tight.
- Attendence at different places is independent.
- Any exposure to the virus in the preceeding week will guarantee a positev test result.

### Model Formualation

We define the model generatively.

Our survey asks whether each sample member visited one of $P$ places, each belonging corresponding to one of $K$ classes (e.g. cinemas, hairdressers), with the class of place $p \in [P]$ is denoted by $C(p):[P] \to [K]$.

Each class $k\in[K]$ has an associated mean transmission rate $\mu_k \sim p(\mu_k)$ and variance $\sigma^2_k \sim p(\sigma^2_k)$. These define the transmission rates $\theta_p$ for each activity $p\in[P]$ through

$$\theta_p \sim \text{expit}\left(\mathcal{N}\left(\text{logit}(\mu_{C(p)})\right) + \sigma^2_{C(p)}\right)$$

This is a standard procedure for adding noise to a value restricted to the range $[0,1]$; we transform the class transmission rate $\mu_C(p)$ to $\mathbb{R}$ using the logit function, add Gaussian noise with variance $\sigma^2_{k}$ before using the expit function to transform  the random variable back to $[0,1]$. 

A small about of algebra (using that the expit function is monotonic) shows that

$$f_{\theta_p}(x) = \frac{1}{x\left(1-x\right)\sqrt{2\pi\sigma^2_{C(p)}}}\exp\left(-\frac{\left(\text{logit}(x) - \text{logit}(\mu_{C(p)})\right)^2}{2\sigma^2_{C(p)}}\right)$$

An interactive toy to visualise this distribution is shown below.

```{r echo=FALSE}
div(style = "display:inline-block; padding-right: 20px",
  sliderInput('mu', "Class Transmission Rate", 0, 1, 0.5, 0.05)
)
div(style = "display:inline-block",
  sliderInput('sigma2', "Class Variance", 0, 10, 2, 0.5)
)

x <- seq(0, 1, length.out = 200)

f_theta <- function(x, mu, sigma2) {
  1 / (x * (1 - x) * sqrt(2 * pi * sigma2)) *
    exp(-(logit(x) - logit(mu))^2 / (2 * sigma2))
}

renderPlot({
  req(input$sigma2 > 0)
  plot(x, f_theta(x, input$mu, input$sigma2), type = 'l',
       xlab = "x", ylab = "f(x)", main = "Density of theta")
  abline(v = input$mu, lty = 2, lwd = 3)
})
```

The data for our model consists of $(\boldsymbol{X}, \boldsymbol{y})$. $\boldsymbol{X}$ is a binary matrix of size $N\times P$ giving the survey responses; that is $\boldsymbol{X}_{n, p} = 1$ if and only if participant $n$ visited place $p$. $\boldsymbol{y}$ is a binary vector of length $n$ giving the test results for each participant using $1$ for positive.

We model $\boldsymbol{y}$ through the latent variable $\boldsymbol{z}$ giving the number of times a participant was exposure to the virus. Naturally, $\boldsymbol{y} = \mathbb{I}\{\boldsymbol{z} > 0\}$.

Likewise, we model $\boldsymbol{z}$ through the latent variable $\boldsymbol{T}$, an $N \times P$ random matrix giving whether transmission occurred for any person at any particular place, and vector $\boldsymbol{t}$ which expresses whether a participant was infected with the Coronavirus from any other source (occuring with probability $\varphi$). We have that

$$
\boldsymbol{T}_{n, p} \sim
\begin{cases}
    0, & \boldsymbol{X}_{n,p} = 0 \\
    \text{Ber}(\theta_p), & \boldsymbol{X}_{n,p} = 1
  \end{cases} \\
\boldsymbol{t} \sim \text{Ber}(\varphi) \\
\boldsymbol{z} = \boldsymbol{T}\cdot \boldsymbol{1} + \boldsymbol{t}
$$

A bit of algebra then shows that

$$
\mathbb{P}\left(\boldsymbol{y}_p = 1\right) = 1 - \left(1-\varphi\right)\prod_{p=1}^P\left(1 - \boldsymbol{X}_{n, p} \theta_p\right)
$$

## Data Generation

With our model defined (and without access to government data), we can test its effectiveness on simulated data.

### Parameters

We define the following model parameters. Transmission rates give how likely an individual is to become infected if they visit a location and occurrence rates given how likely they are to visit somewhere in the first place.

```{r}
NUM_EACH_CLASS = c(3, 4, 1, 4)  # number of places of each class
N = 1000  # number of participants
SEED = 1729  # random seed
CLASS_TRNS_RATE_ALPHA = 2  # shape parameter for base class transmission rates
CLASS_TRNS_RATE_BETA = 10  # shape parameter for base class transmission rates
CLASS_TRNS_VAR = 100 # shape parameter for base class transmission variances
CLASS_OCUR_RATE_ALPHA = 2  # shape parameter for base class occurrence rates
CLASS_OCUR_RATE_BETA = 5  # shape parameter for base class occurrence rates
CLASS_OCUR_VAR = 50 # shape parameter for base class occurrence variances
BASE_TRNS_RATE_ALPHA = 4  # shape parameter for base transmission rates
BASE_TRNS_RATE_BETA = 8  # shape parameter for base transmission rates
```

We can then derive some convenience parameters from this.

```{r}
K <- length(NUM_EACH_CLASS)
P <- sum(NUM_EACH_CLASS)
Cp <- rep(1:K, NUM_EACH_CLASS)
```

### Ground Truth

We now sample the model ground truth for a sensible distribution. We choose the base transmission and occurrence rates for each class from beta distributions and the base variances from inverse gamma distributions with scale of 1. We also sample the base transmission rate from a beta distribution. After this, we generate transmission and occurrence rates for each location, using the procedure described in [Model Formulation](#model-formulation).

```{r}
set.seed(SEED)

add_noise <- function(class_sizes, alpha, beta, var_alpha) {
  n_classes <- length(class_sizes)
  base_rates <- rbeta(n_classes, alpha, beta)
  class_variances <- 1 / rgamma(n_classes, var_alpha)
  rates <- unlist(pmap(
    list(m = class_sizes,
         mu = base_rates,
         sigma2 = class_variances),
    function(m, mu, sigma2) {
      expit(logit(mu) + rnorm(m, 0, sqrt(sigma2)))
    }
  ))
  rates
}

true_transmission_rates <- add_noise(
  NUM_EACH_CLASS, CLASS_TRNS_RATE_ALPHA, CLASS_TRNS_RATE_BETA, CLASS_TRNS_VAR
)
true_occurrence_rates <- add_noise(
  NUM_EACH_CLASS, CLASS_OCUR_RATE_ALPHA, CLASS_OCUR_RATE_BETA, CLASS_OCUR_VAR
)

true_base_rate <- rbeta(1, BASE_TRNS_RATE_ALPHA, BASE_TRNS_RATE_BETA)
```

We visualise the rates below.

```{r echo=FALSE}
renderPlot({
  tibble(location = 1:P, class = Cp,
         trans_rate = true_transmission_rates,
         occur_rate = true_occurrence_rates) %>%
    gather(trans_rate, occur_rate, key = 'type', value = 'rate') %>%
    ggplot(aes(x = factor(location), y = rate, fill = factor(class))) +
      geom_col(col = 'black', alpha = 0.7) +
      facet_grid(type ~ ., labeller = labeller(
        type = c('occur_rate' = "Occurrences", 'trans_rate' = "Transmission")
      )) +
      labs(x = "Location", y = "Rate", fill = "Class") +
      scale_y_continuous(expand = c(0, 0, 0, 0.1)) +
      scale_fill_viridis_d(option = "plasma")
})
```

### Simulating Data

We can now use these ground truth rates to simulate our data. We represent occurrences using columns beginning with 'O' and likewise for transmissions with 'T'. 

```{r}
X <- pmap_dfc(
  list(
    true_occurrence_rates,
    true_transmission_rates,
    1:P),
  function(occur_rate, trans_rate, p) {
    occurence <- rbinom(N, 1, occur_rate)
    transmission <- occurence * rbinom(N, 1, trans_rate)
    tibble(O = occurence, T = transmission) %>%
      rename_all(~paste0(., p))
}) %>%
  mutate(T0 = rbinom(N, 1, true_base_rate))

# Latent variable
z <- X %>%
  select(starts_with('T')) %>%
  rowSums()

# Observed variables
X <- X %>%
  select(starts_with('O'))
y <- as.integer(z > 0)
```

## Modelling

### Model Definition

We are now ready to build our Stan model. This essentially comes down to considering the unnormalised log-likelihood (denoted `target` in Stan) from our derivations above. We use weak priors on all variables.

```{stan output.var='censored_poisbinom_mod'}
functions {
  real expit_gaussian_lpdf(real y, real logit_mu, real sigma2) {
    return log(1 / y + 1 / (1 - y)) - (logit(y) - logit_mu)^2 / (2 * sigma2);
  }
}
data {
  int<lower=0> N;                            // number of observations
  int<lower=0> P;                            // number of places
  int<lower=0> K;                            // number of classes
  int<lower=0, upper=1> X[N,P];              // activity occurrences
  int<lower=0, upper=1> y[N];                // transmission (tested positive)
  int<lower=1, upper=K> c[P];                // place classes
}
parameters {
  real<lower=0, upper=1> theta[P];           // transmission rates
  real<lower=0, upper=1> mu[K];              // class transmission means
  real<lower=0> sigma2[K];                   // class transmission variances
  real<lower=0, upper=1> rho;                // underlying risk
}
transformed parameters {
  // Pre-computation for efficiency
  real log1m_theta[P] = log1m(theta);
  real log1m_rho = log1m(rho);
  real logit_mu[K] = logit(mu);
}
model {
  // Priors
  mu ~ beta(1, 5);
  sigma2 ~ inv_gamma(10, 1);
  rho ~ beta(1, 3);
  // Likelihood (classes)
  for (p in 1:P) {
    theta[p] ~ expit_gaussian(logit_mu[c[p]], sigma2[c[p]]);
  }
  // Likelihood (observations)
  for (n in 1:N) {
    real s = 0.0;
    for (p in 1:P) {
      if (X[n,p] == 1) {
        s += log1m_theta[p];
      }
    }
    s += log1m_rho;
    if (y[n] == 1) {
      target += log1m_exp(s);
    } else {
      target += s;
    }
  }
}
```

### Fitting

We first specify parameters for the fitting then process, then we are ready to fit the model.

```{r}
ITER = 1000  # total iterations
WARMUP = 250  # burn-in period
```

```{r results=FALSE}
fit <- sampling(censored_poisbinom_mod,
                iter = ITER, warmup = WARMUP, chains = NUM_CORES, 
                data = list(N = N, P = P, K = K, X = X, y = y, c = Cp))
```

### Inference and Evaluation

We can compare the ground truth parameter values to our estimates.

```{r echo=FALSE}
renderTable({
  summary(fit)$summary %>%
    as_tibble(rownames = NA) %>%
    rownames_to_column('parameter') %>%
    # Ignore transformed parameters and log-likelihood
    filter(str_detect(parameter, '^[theta|rho]')) %>%
    mutate(truth = c(true_transmission_rates, true_base_rate)) %>%
    relocate(truth, .after = parameter) %>%
    select(-c(`25%`:`75%`))
})
```

Finally, we ensure convergence by comparing the parameter sampling distribution across chains.

```{r echo=FALSE, fig.width=8, fig.height=6}
renderPlot({
  as.data.frame(fit) %>%
    select(starts_with(c('theta', 'rho'))) %>%
    mutate(chain = rep(1:NUM_CORES, each = (ITER - WARMUP))) %>%
    relocate(chain, 1) %>%
    gather(2:last_col(), key = 'parameter', value = 'estimate') %>%
    ggplot(aes(x = estimate)) +
      geom_density(aes(col = factor(chain))) +
      scale_x_continuous(limits = c(0, 1)) +
      facet_wrap(~parameter) +
      labs(x = 'Estimate', y = 'Density', col = 'Chain') +
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
})
```
